{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Durga22-amie/-vqe-cancer-segmentation-/blob/main/mask_and_vqe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tKF15GWGdXbB",
        "outputId": "19cceafe-6366-4503-d192-39393a3d3b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
            "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.5\n",
            "    Uninstalling numpy-2.3.5:\n",
            "      Successfully uninstalled numpy-2.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.6\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.2.6)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install opencv-python\n",
        "!{sys.executable} -m pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1053
        },
        "id": "Pqp0ceR5en3z",
        "outputId": "230cc1cf-45d8-4d2d-da69-0e387e79750a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.2.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.1.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "0985d340ece94d7ab5df26b2f4f70e34"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow numpy opencv-python tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DrHJKFXsd8mM",
        "outputId": "4468574a-2194-41cb-f18b-de2b9ac16960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6TSs5fgaeB4S"
      },
      "outputs": [],
      "source": [
        "input_folder = \"/content/drive/My Drive/dataset/YES NO Dataset\"\n",
        "output_folder = \"/content/drive/My Drive/dataset/YES NO Dataset masks\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lhn0eseTdiJb",
        "outputId": "21d0b463-77ad-41c6-b31a-8195dcc42b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_8_yes.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_4_yes.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_6_yes.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_5_yes.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_9_yes (2).jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_3_yes.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_2_yes.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_1_yes.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_10_no (2).jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_7_yes.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_8_no.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_5_no.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_9_no.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_7_no.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_10_no (1).jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_3_no.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_6_no.jpg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_2_no.jpeg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_1_no.jpeg\n",
            "Saved mask: /content/drive/My Drive/dataset/YES NO Dataset masks/mask_4_no.jpg\n",
            "\n",
            "All masks generated successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------------------\n",
        "# PATHS (CHANGE THESE)\n",
        "# ---------------------------------------\n",
        "input_folder = \"/content/drive/My Drive/dataset/YES NO Dataset\"\n",
        "output_folder = \"/content/drive/My Drive/dataset/YES NO Dataset masks\"      # Folder where masks will be saved\n",
        "\n",
        "# ---------------------------------------\n",
        "# CREATE OUTPUT FOLDER IF NOT EXISTS\n",
        "# ---------------------------------------\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# ---------------------------------------\n",
        "# PROCESS EACH IMAGE\n",
        "# ---------------------------------------\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tif\", \".bmp\")):\n",
        "\n",
        "        # Full input path\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "\n",
        "        # Read image in grayscale\n",
        "        img = cv2.imread(img_path, 0)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Skipping {filename}, cannot read image.\")\n",
        "            continue\n",
        "\n",
        "        # ---------------------------------------\n",
        "        # Apply threshold to create mask\n",
        "        # You can change 120 to 100 / 150\n",
        "        # ---------------------------------------\n",
        "        _, mask = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        # Save mask\n",
        "        mask_path = os.path.join(output_folder, f\"mask_{filename}\")\n",
        "        cv2.imwrite(mask_path, mask)\n",
        "\n",
        "        print(f\"Saved mask: {mask_path}\")\n",
        "\n",
        "print(\"\\nAll masks generated successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "3Rt7R9dPhaM6",
        "outputId": "e0e77deb-42ee-49d8-e14e-19a6a66b7f2e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1513289574.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Predict with U-Net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Threshold output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "input_folder = \"/content/drive/My Drive/dataset/YES NO Dataset\"\n",
        "output_folder = \"/content/drive/My Drive/dataset/YES NO Dataset_UNET_masks\"\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "def preprocess(img):\n",
        "    img = cv2.resize(img, (256, 256))\n",
        "    img = img / 255.0\n",
        "    return img.reshape(1, 256, 256, 1)\n",
        "\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "\n",
        "        path = os.path.join(input_folder, filename)\n",
        "\n",
        "        # Read grayscale MRI\n",
        "        img = cv2.imread(path, 0)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        # Preprocess\n",
        "        inp = preprocess(img)\n",
        "\n",
        "        # Predict with U-Net\n",
        "        pred = model.predict(inp)[0, :, :, 0]\n",
        "\n",
        "        # Threshold output\n",
        "        mask = (pred > 0.5).astype(np.uint8) * 255\n",
        "\n",
        "        # Resize mask back to original size\n",
        "        mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
        "\n",
        "        # Save mask\n",
        "        out_path = os.path.join(output_folder, \"mask_\" + filename)\n",
        "        cv2.imwrite(out_path, mask)\n",
        "\n",
        "        print(\"Saved:\", out_path)\n",
        "\n",
        "print(\"\\nüéâ All U-Net masks generated successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the folder containing your masks\n",
        "masks_directory_path = \"/content/drive/My Drive/dataset/YES NO Dataset_UNET_masks\"\n",
        "\n",
        "# !!! CORRECTED LINE !!!\n",
        "# Use os.path.join to build the search pattern for ALL .npy files inside the directory\n",
        "search_pattern = os.path.join(masks_directory_path, \"*.npy\")\n",
        "# If they are images, use \"*.png\" or \"*.jpg\"\n",
        "\n",
        "list_of_masks = []\n",
        "mask_files = glob.glob(search_pattern)\n",
        "\n",
        "if not mask_files:\n",
        "    print(f\"Error: No files found using pattern: {search_pattern}. Check your path and file extensions.\")\n",
        "else:\n",
        "    print(f\"Found {len(mask_files)} files. Loading data...\")\n",
        "    for file_path in mask_files:\n",
        "        mask_data = np.load(file_path)\n",
        "\n",
        "        # Add validation and append to list\n",
        "        if mask_data.shape == (218, 180):\n",
        "            list_of_masks.append(mask_data)\n",
        "        else:\n",
        "            print(f\"Skipping file {file_path} due to incorrect shape {mask_data.shape}\")\n",
        "\n",
        "# NOW PASTE THE REST OF THE VQE PROCESSING CODE HERE\n",
        "# ... (the rest of the function definition and main processing loop from before) ...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LsJS6nJx0VuW",
        "outputId": "d6f0c41b-e0b2-4012-c415-2cd639cf7f88"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: No files found using pattern: /content/drive/My Drive/dataset/YES NO Dataset_UNET_masks/*.npy. Check your path and file extensions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "e9XadoJzzcMm",
        "outputId": "772034c5-1a5a-49bb-966b-db27c1466bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting VQE feature extraction for 0 masks...\n",
            "\n",
            "Finished extraction.\n",
            "Collected 0 VQE features.\n",
            "Features: []\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from qiskit.circuit.library import pauli_feature_map, efficient_su2\n",
        "from qiskit.primitives import StatevectorEstimator\n",
        "from qiskit_algorithms import VQE\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from qiskit.quantum_info import SparsePauliOp\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "NUM_FEATURES = 8 # Number of features to reduce to via PCA (and number of qubits)\n",
        "REPS_FEATURE_MAP = 1\n",
        "REPS_ANSATZ = 2\n",
        "MAX_VQE_ITERATIONS = 100\n",
        "# ---------------------\n",
        "\n",
        "# Assume 'list_of_masks' is a list containing your 20 U-Net NumPy arrays\n",
        "# Example placeholder data (replace this with your actual mask loading code):\n",
        "# list_of_masks = [np.random.rand(218, 180) for _ in range(20)]\n",
        "# (Add your code here to load the real mask data)\n",
        "list_of_masks = [] # Initialize your list here\n",
        "\n",
        "def extract_vqe_feature(mask_2d: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Processes a single (218, 180) mask through PCA and VQE to extract one feature value.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Classical Dimensionality Reduction (PCA)\n",
        "    n_pixels = mask_2d.size\n",
        "    flattened_data = mask_2d.reshape(1, n_pixels)\n",
        "\n",
        "    pca = PCA(n_components=NUM_FEATURES)\n",
        "    reduced_features = pca.fit_transform(flattened_data).flatten()\n",
        "\n",
        "    # 2. Scale the data for angle encoding (0 to 2*pi range)\n",
        "    if reduced_features.max() == reduced_features.min():\n",
        "        scaled_input_vector = np.zeros(NUM_FEATURES)\n",
        "    else:\n",
        "        scaled_input_vector = (reduced_features - reduced_features.min()) / \\\n",
        "                              (reduced_features.max() - reduced_features.min()) * (2 * np.pi)\n",
        "\n",
        "    # 3. Create the Quantum Circuit Components\n",
        "    num_qubits = NUM_FEATURES\n",
        "    feature_map = pauli_feature_map(feature_dimension=num_qubits, reps=REPS_FEATURE_MAP, entanglement='linear')\n",
        "    ansatz = efficient_su2(num_qubits=num_qubits, reps=REPS_ANSATZ, entanglement='linear')\n",
        "    observable = SparsePauliOp.from_list([('Z'*num_qubits, 1.0)])\n",
        "\n",
        "    # 4. Bind the data and combine circuits\n",
        "    encoded_circuit = feature_map.assign_parameters(scaled_input_vector)\n",
        "    full_circuit = encoded_circuit.compose(ansatz)\n",
        "\n",
        "    # 5. Setup and Run VQE\n",
        "    estimator = StatevectorEstimator()\n",
        "    optimizer = COBYLA(maxiter=MAX_VQE_ITERATIONS)\n",
        "    vqe = VQE(estimator, ansatz, optimizer=optimizer)\n",
        "\n",
        "    # Run VQE and capture the optimal value\n",
        "    result = vqe.compute_minimum_eigenvalue(observable)\n",
        "\n",
        "    return result.optimal_value\n",
        "\n",
        "# --- Main loop to process all masks ---\n",
        "\n",
        "vqe_features = []\n",
        "\n",
        "print(f\"Starting VQE feature extraction for {len(list_of_masks)} masks...\")\n",
        "\n",
        "for i, mask in enumerate(list_of_masks):\n",
        "    print(f\"Processing mask {i+1}/{len(list_of_masks)}...\")\n",
        "    try:\n",
        "        feature_value = extract_vqe_feature(mask)\n",
        "        vqe_features.append(feature_value)\n",
        "        print(f\"  -> Extracted feature: {feature_value}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing mask {i+1}: {e}\")\n",
        "\n",
        "print(\"\\nFinished extraction.\")\n",
        "print(f\"Collected {len(vqe_features)} VQE features.\")\n",
        "print(f\"Features: {vqe_features}\")\n",
        "\n",
        "# Now you can save these features to a CSV or use them in your classical classifier\n",
        "# np.savetxt(\"vqe_extracted_features.csv\", vqe_features, delimiter=\",\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8TS2YUMuoERw"
      },
      "outputs": [],
      "source": [
        "from qiskit.circuit.library import efficient_su2\n",
        "ansatz = efficient_su2(num_qubits=8, reps=3, entanglement=\"full\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AiBquZPrnWO4"
      },
      "outputs": [],
      "source": [
        "def mask_to_hamiltonian(vec):\n",
        "    terms = []\n",
        "    for i, value in enumerate(vec):\n",
        "        pauli_string = 'I'*i + 'Z' + 'I'*(8-i-1)\n",
        "        weight = float(value)\n",
        "        terms.append((pauli_string, weight))\n",
        "    return SparsePauliOp.from_list(terms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1HU56Cs8nOFx"
      },
      "outputs": [],
      "source": [
        "def load_masks(folder):\n",
        "    files = sorted(os.listdir(folder))\n",
        "    masks = []\n",
        "    for f in files:\n",
        "        if f.endswith(\".png\") or f.endswith(\".jpg\"):\n",
        "            img = Image.open(os.path.join(folder, f)).convert(\"L\")\n",
        "            img = img.resize((16, 16))\n",
        "            vec = np.array(img).flatten() / 255.0\n",
        "            masks.append(vec)\n",
        "    return masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XhhLlyYvk_5p",
        "outputId": "35e94df7-8dcc-4af6-f349-8d3f57f01b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask loaded: (218, 180)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "mask_path = \"/content/drive/My Drive/dataset/YES NO Dataset masks/mask_1_yes.jpg\"\n",
        "\n",
        "# Load as grayscale\n",
        "mask_img = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "if mask_img is None:\n",
        "    print(\"Error: mask image not found!\")\n",
        "else:\n",
        "    print(\"Mask loaded:\", mask_img.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gZ_lTyGUlO8Z",
        "outputId": "6d5be781-8105-4e34-d1a5-6396ffe1143c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector length: 256\n"
          ]
        }
      ],
      "source": [
        "mask_16 = cv2.resize(mask_img, (16, 16))\n",
        "vec256 = mask_16.flatten()\n",
        "\n",
        "print(\"Vector length:\", len(vec256))    # should be 256\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndLP3Q3klXPt"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# The original problem was trying to fit PCA with n_components=8 on a single sample.\n",
        "# vec256.reshape(1, -1) results in (1 sample, 256 features).\n",
        "# PCA requires n_components <= n_samples.\n",
        "\n",
        "pca = PCA(n_components=8)\n",
        "\n",
        "# To make PCA fit with n_components=8, we need a dataset with at least 8 samples.\n",
        "# For demonstration, we'll create a small synthetic dataset based on vec256.\n",
        "# In a real scenario, you would gather multiple 'vec256' arrays from different masks.\n",
        "num_synthetic_samples = 10 # Must be >= n_components\n",
        "synthetic_data = np.array([vec256 + np.random.normal(0, 5, vec256.shape) for _ in range(num_synthetic_samples)])\n",
        "\n",
        "# Fit PCA on the synthetic dataset\n",
        "pca.fit(synthetic_data)\n",
        "\n",
        "# Then transform the original vec256 using the fitted PCA model\n",
        "vec8 = pca.transform(vec256.reshape(1, -1))[0]\n",
        "\n",
        "print(\"8-dimensional vector:\", vec8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odi4JPp0lkdR"
      },
      "outputs": [],
      "source": [
        "from qiskit.quantum_info import SparsePauliOp\n",
        "\n",
        "terms = []\n",
        "n_qubits = 8\n",
        "\n",
        "for i, value in enumerate(vec8):\n",
        "    pauli = 'I'*i + 'Z' + 'I'*(n_qubits-i-1)\n",
        "    terms.append((pauli, float(value)))\n",
        "\n",
        "H = SparsePauliOp.from_list(terms)\n",
        "print(H)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6V_qK39mGhJ"
      },
      "outputs": [],
      "source": [
        "! pip install qiskit-aer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ4rw72auBnF"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall qiskit qiskit-aer qiskit-algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxZgZMBctI2p"
      },
      "outputs": [],
      "source": [
        "\n",
        "from qiskit.circuit.library import efficient_su2\n",
        "from qiskit.primitives import Estimator\n",
        "from qiskit_algorithms.minimum_eigensolvers import VQE\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "from qiskit.quantum_info import SparsePauliOp\n",
        "\n",
        "# Example 8-qubit Hamiltonian\n",
        "pauli_strings = ['ZIIIIIII', 'IZIIIIII', 'IIZIIIII', 'IIIZIIII',\n",
        "                 'IIIIZIII', 'IIIIIZII', 'IIIIIIZI', 'IIIIIIIZ']\n",
        "coeffs = [-0.6487, -2.6251, 0.9424, 0.0924, -0.2661, -3.2568, 0.9093, -4.3029]\n",
        "\n",
        "# Add small X terms to avoid AlgorithmError\n",
        "pauli_strings += ['XIIIIIII', 'IXIIIIII']\n",
        "coeffs += [0.05, 0.05]\n",
        "\n",
        "H = SparsePauliOp(pauli_strings, coeffs=coeffs)\n",
        "\n",
        "# Ansatz\n",
        "ansatz = efficient_su2(num_qubits=8, reps=3, entanglement='full')\n",
        "\n",
        "# Estimator + optimizer\n",
        "estimator = Estimator()\n",
        "optimizer = SPSA(maxiter=50)\n",
        "\n",
        "# VQE\n",
        "vqe = VQE(ansatz=ansatz, optimizer=optimizer, estimator=estimator)\n",
        "\n",
        "# Run\n",
        "result = vqe.compute_minimum_eigenvalue(operator=H)\n",
        "print(\"VQE Energy:\", result.eigenvalue.real)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8lf2eBZgo9Nw",
        "outputId": "34039d98-366e-4708-c940-96b4e557a854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit==0.41.0\n",
            "  Using cached qiskit-0.41.0.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting qiskit-terra==0.23.1 (from qiskit==0.41.0)\n",
            "  Using cached qiskit-terra-0.23.1.tar.gz (4.9 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting qiskit-aer==0.11.2 (from qiskit==0.41.0)\n",
            "  Using cached qiskit-aer-0.11.2.tar.gz (6.5 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install qiskit==0.41.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-4Iqnf7Tpb9B",
        "outputId": "f5a31109-bcae-44c4-f4ef-946ae4a22ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qiskit-algorithms in /usr/local/lib/python3.12/dist-packages (0.4.0)\n",
            "Requirement already satisfied: qiskit>=1.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-algorithms) (2.2.3)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.12/dist-packages (from qiskit-algorithms) (1.16.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from qiskit-algorithms) (2.1.3)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit-algorithms) (0.17.1)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit-algorithms) (0.4.0)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit-algorithms) (5.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit-algorithms) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install qiskit-algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LqFjx8UxxsAZ",
        "outputId": "6bb35240-0731-4fea-9838-60253835d923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VQE Optimal Value: -0.7363146854558065\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Import the functions instead of the classes\n",
        "from qiskit.circuit.library import pauli_feature_map, efficient_su2\n",
        "from qiskit.primitives import StatevectorEstimator\n",
        "from qiskit_algorithms import VQE\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from qiskit.quantum_info import SparsePauliOp\n",
        "\n",
        "# 1. Define your 8-dimensional data\n",
        "input_vector = np.array([-0.64876986, -2.62517237, 0.94244815, 0.09240809,\n",
        "                         -0.2661078, -3.25682053, 0.90935779, -4.30299167])\n",
        "\n",
        "num_qubits = len(input_vector)\n",
        "\n",
        "# 2. Scale the data for angle encoding\n",
        "scaled_input_vector = (input_vector - input_vector.min()) / (input_vector.max() - input_vector.min()) * (2 * np.pi)\n",
        "\n",
        "# 3. Create the Feature Map using the FUNCTION `pauli_feature_map`\n",
        "# This function returns a QuantumCircuit object directly\n",
        "feature_map = pauli_feature_map(feature_dimension=num_qubits, reps=1, entanglement='linear')\n",
        "encoded_circuit = feature_map.assign_parameters(scaled_input_vector)\n",
        "\n",
        "# 4. Create the Ansatz using the FUNCTION `efficient_su2`\n",
        "ansatz = efficient_su2(num_qubits=num_qubits, reps=2, entanglement='linear')\n",
        "full_circuit = encoded_circuit.compose(ansatz)\n",
        "\n",
        "# 5. Define an Observable (Cost function/Hamiltonian)\n",
        "observable = SparsePauliOp.from_list([('Z'*num_qubits, 1.0)])\n",
        "\n",
        "# 6. Set up and Run VQE\n",
        "estimator = StatevectorEstimator()\n",
        "optimizer = COBYLA(maxiter=100)\n",
        "\n",
        "vqe = VQE(estimator, ansatz, optimizer=optimizer)\n",
        "\n",
        "# The observable is passed as the first POSITIONAL argument\n",
        "result = vqe.compute_minimum_eigenvalue(observable, aux_operators=[])\n",
        "\n",
        "print(f\"VQE Optimal Value: {result.optimal_value}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgi5xusQa9DcIon1ntFrN3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}